## Related Works

In the related work section of this paper, different approaches to optimizing decision trees and the challenges associated with finding the best split functions are explored. For decision trees, finding the best split function is difficult becuase the process is NP-complete due to the sequential and discrete nature of decisions. As such, finding an efficient alternative to greedy approaches remains unknown. Non-greedy optimization is addressed by Bennett, who introduced a multi-linear programming method that produces decision trees with higher classification accuracy than standard greedy trees. However, this method is limited to binary classification with 0-1 loss and suffers from high computational complexity. Another approach is the training of decision forests in an online setting, as described in the work involving Mondrian Processes. This method extends trees as new data arrives, contrasting with naive incremental tree growing. Soft splits are explored through the Hierarchical Mixture of Experts model, which uses gradual transitions instead of binary decisions. This allows for training based on log-likelihood, suitable to numerical optimization like Expectation Maximization. However, soft splits require evaluating all or most experts for each data point, thus diminishing the computational advantage of decision trees. Murthy and Salzberg discuss potential issues with non-greedy learning, such as overfitting when the empirical loss is minimized without regularization. They suggest limits on tree depth and instance counts below which branches arenâ€™t extended to prevent overfitting. Bennett and Blue propose using max-margin frameworks and SVMs at split nodes to overcome overfitting.
