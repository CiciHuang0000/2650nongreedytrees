import Image from 'next/image'
# Experiments

The experiments in this paper were conducted on multiple benchmark datasets from the LibSVM collection, specifically focusing on the SensIT, Connect4, Protein, and MNIST datasets. The methodology involved dividing each dataset into different subsets for training, validation, and testing. The paper compares the proposed non-greedy learning algorithm for decision trees against several baseline methods. These include traditional decision trees based on information gain and OC1 trees that use coordinate descent for optimization. The non-greedy trees were also compared to the CO2 algorithm, which is an upper bound approach that constructs decision trees by selecting hyperplanes based on information gain. The depth of the trees, varying from 6 to 18, was a critical variable, controlled to assess performance impact. Each method was individually tuned for different tree depths. A notable aspect of the method is the use of a regularization parameter, $\nu$, which controls the tightness of the margin in the trees and influences how much the trees are pruned. Smaller values of $\nu$ result in fewer leaves by pruning more aggressively.

**1:** Shows training and test accuracy for trees with different depths, highlighting the performance of non-greedy trees compared to baselines.
<Image src="/Figure1.png" alt="conv" width={800} height={500} style={
    {
        marginTop: 10,
        marginBottom: 10
    }
}/>

**2:** Illustrates the impact of the regularization parameter on tree structure, useful for understanding how parameter $\nu$ affects pruning.
<Image src="/Figure2.png" alt="conv" width={800} height={500} style={
    {
        marginTop: 10,
        marginBottom: 10
    }
}/>

**3:** Compares the training time required for Connect4 dataset using different inference methods, demonstrating efficiency gains.
<Image src="/Figure3.png" alt="conv" width={800} height={500} style={
    {
        marginTop: 10,
        marginBottom: 10
    }
}/>

These results collectively emphasize the efficacy of non-greedy optimization in building decision trees that not only perform better but also are more efficient in terms of computational resources and time.
